{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "937f7d41-ef26-404c-8491-349b19c9db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "from importlib import util\n",
    "from pathlib import Path\n",
    "import triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8422322b-4423-4514-991c-83b3ae391606",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_fpath = Path('./compile_test.py')\n",
    "kernel_name = 'add_kernel'\n",
    "signature = '*fp32, *fp32, *fp32, i32, 1024'\n",
    "num_warps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfcf87b1-e423-4b5a-af1d-00a0ac411d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JITFunction(compile_test:add_kernel)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec = util.spec_from_file_location(code_fpath.stem, code_fpath)\n",
    "mod = util.module_from_spec(spec)\n",
    "spec.loader.exec_module(mod)\n",
    "kernel = getattr(mod, kernel_name)\n",
    "kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0a31750-29e6-493b-8dbf-570f2bdf6f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_args = signature.replace(',', '').split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a25b62d0-616d-4703-acc8-def8f5db9e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'78484a0b'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_ = hashlib.sha256()\n",
    "hash_.update(' '.join(sig_args).encode())\n",
    "sig_hash = hash_.hexdigest()[:8]\n",
    "sig_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd47e788-6cf5-46c7-8cfe-94256cb19310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hints = {i: int(arg.split(':')[1]) for i, arg in enumerate(sig_args) if ':' in arg}\n",
    "assert all(h in [1, 16] for h in hints.values())\n",
    "hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e94ce3b-0c41-41a2-992a-f8c1b06f0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg2num(arg):\n",
    "    try:\n",
    "        num = int(arg)\n",
    "        return num\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        num = float(arg)\n",
    "        return num\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f1c906-46ae-4de6-817c-84f671a7e8d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({4: 1024}, {0: '*fp32', 1: '*fp32', 2: '*fp32', 3: 'i32'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constexprs = {i: num for i, arg in enumerate(sig_args) if (num := arg2num(arg)) is not None}\n",
    "# sig_canon = {i: arg for i, arg in enumerate(sig_args) if i not in constexprs.keys()}\n",
    "constexprs = dict()\n",
    "sig_canon = dict()\n",
    "\n",
    "for i, arg in enumerate(sig_args):\n",
    "    if (num := arg2num(arg)) is None:\n",
    "        sig_canon[i] = arg\n",
    "    else:\n",
    "        constexprs[i] = num\n",
    "constexprs, sig_canon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a6211cc-a34c-4063-a68e-307340960d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instance_descriptor(divisible_by_16=[], equal_to_1=[])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = triton.compiler.instance_descriptor(\n",
    "    divisible_by_16=[i for i, h in hints.items() if h == 16],\n",
    "    equal_to_1=[i for i, h in hints.items() if h == 1]\n",
    ")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c22d5ae-c79d-4bd1-9bb3-17d9ba7ac027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccinfo = triton.compile(kernel, signature=sig_canon, constants=constexprs, configs=[config], num_warps=num_warps)\n",
    "# ccinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a3dad5a-d28b-46c8-bd75-ae0597102845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from triton._C.libtriton.triton import ir\n",
    "import triton.compiler.compiler as ttc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e439243-a062-4e71-9531-9c9c7581c2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = ttc.get_architecture_descriptor(None)\n",
    "num_stages = 2 if arch < 75 else 3\n",
    "context = ir.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d45315c-a500-4539-b58c-69a3bfac25ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "module {\n",
      "  tt.func public @add_kernel_0123(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32) attributes {noinline = false} {\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %c1024_i32 = arith.constant 1024 : i32\n",
      "    %1 = arith.muli %0, %c1024_i32 : i32\n",
      "    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n",
      "    %3 = tt.splat %1 : (i32) -> tensor<1024xi32>\n",
      "    %4 = arith.addi %3, %2 : tensor<1024xi32>\n",
      "    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32>\n",
      "    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32>\n",
      "    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n",
      "    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n",
      "    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n",
      "    %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n",
      "    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n",
      "    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n",
      "    %13 = arith.addf %9, %12 : tensor<1024xf32>\n",
      "    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n",
      "    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n",
      "    tt.store %15, %13, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n",
      "    tt.return\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ttir = ttc.ast_to_ttir(kernel, sig_canon, config, constexprs, debug=True, arch=arch)\n",
    "ttir.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d7b94ba-c45b-4c47-ae9c-f36e5c85a543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "module {\n",
      "  tt.func public @add_kernel_0123(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32) attributes {noinline = false} {\n",
      "    %c1024_i32 = arith.constant 1024 : i32\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %1 = arith.muli %0, %c1024_i32 : i32\n",
      "    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32>\n",
      "    %3 = tt.splat %1 : (i32) -> tensor<1024xi32>\n",
      "    %4 = arith.addi %3, %2 : tensor<1024xi32>\n",
      "    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32>\n",
      "    %6 = arith.cmpi slt, %4, %5 : tensor<1024xi32>\n",
      "    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n",
      "    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n",
      "    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n",
      "    %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n",
      "    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n",
      "    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32>\n",
      "    %13 = arith.addf %9, %12 : tensor<1024xf32>\n",
      "    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>>\n",
      "    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32>\n",
      "    tt.store %15, %13, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32>\n",
      "    tt.return\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ttir_opt = ttc.optimize_ttir(ttir, arch)\n",
    "ttir_opt.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82fbcc71-a32b-415f-bc5e-ea6acb6ffec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "module attributes {\"triton_gpu.num-warps\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n",
      "  tt.func public @add_kernel_0123(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32) attributes {noinline = false} {\n",
      "    %c1024_i32 = arith.constant 1024 : i32\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %1 = arith.muli %0, %c1024_i32 : i32\n",
      "    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %4 = arith.addi %3, %2 : tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %6 = \"triton_gpu.cmpi\"(%4, %5) <{predicate = 2 : i64}> : (tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %13 = arith.addf %9, %12 : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    tt.store %15, %13, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    tt.return\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ttgir = ttc.ttir_to_ttgir(ttir_opt, num_warps)\n",
    "ttgir.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4b9e4e1-5819-4bfc-9436-c7892550fbf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "module attributes {\"triton_gpu.num-warps\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n",
      "  tt.func public @add_kernel_0123(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32) attributes {noinline = false} {\n",
      "    %c1024_i32 = arith.constant 1024 : i32\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %1 = arith.muli %0, %c1024_i32 : i32\n",
      "    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %4 = arith.addi %3, %2 : tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %6 = \"triton_gpu.cmpi\"(%4, %5) <{predicate = 2 : i64}> : (tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %9 = tt.load %8, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %10 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %11 = tt.addptr %10, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %12 = tt.load %11, %6 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %13 = arith.addf %9, %12 : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %14 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %15 = tt.addptr %14, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    tt.store %15, %13, %6 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    tt.return\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "ttgir_opt = ttc.optimize_ttgir(ttgir, num_stages, arch)\n",
    "ttgir_opt.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18f7a386-f992-4392-bb7d-958bd3f75f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "module attributes {\"triton_gpu.num-warps\" = 1 : i32, \"triton_gpu.threads-per-warp\" = 32 : i32} {\n",
      "  tt.func public @add_kernel_0123(%arg0: !tt.ptr<f32>, %arg1: !tt.ptr<f32>, %arg2: !tt.ptr<f32>, %arg3: i32) attributes {noinline = false} {\n",
      "    %c1024_i32 = arith.constant 1024 : i32\n",
      "    %0 = tt.get_program_id x : i32\n",
      "    %1 = arith.muli %0, %c1024_i32 : i32\n",
      "    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %3 = tt.splat %1 : (i32) -> tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %4 = arith.addi %3, %2 : tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %5 = tt.splat %arg3 : (i32) -> tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %6 = \"triton_gpu.cmpi\"(%4, %5) <{predicate = 2 : i64}> : (tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %7 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %8 = tt.addptr %7, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %9 = triton_gpu.convert_layout %8 : (tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %10 = triton_gpu.convert_layout %6 : (tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %11 = tt.load %9, %10 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %12 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %13 = tt.addptr %12, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %14 = triton_gpu.convert_layout %13 : (tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %15 = triton_gpu.convert_layout %6 : (tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %16 = tt.load %14, %15 {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %17 = arith.addf %11, %16 : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %18 = tt.splat %arg2 : (!tt.ptr<f32>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %19 = tt.addptr %18, %4 : tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>, tensor<1024xi32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %20 = triton_gpu.convert_layout %19 : (tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024x!tt.ptr<f32>, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %21 = triton_gpu.convert_layout %17 : (tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    %22 = triton_gpu.convert_layout %6 : (tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>) -> tensor<1024xi1, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    tt.store %20, %21, %22 {cache = 1 : i32, evict = 1 : i32} : tensor<1024xf32, #triton_gpu.blocked<{sizePerThread = [1], threadsPerWarp = [32], warpsPerCTA = [1], order = [0]}>>\n",
      "    tt.return\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "pm = ir.pass_manager(ttgir.context)\n",
    "pm.enable_debug()\n",
    "pm.add_tritongpu_coalesce_pass()\n",
    "pm.run(ttgir)\n",
    "ttgir.dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
