# Read List Dump

I have collected so many papers and articles recently that I forgot why I wanted to read them.
Here is my attempt to gather them all here, group them into topics, and prioritize groups based on topics.


## Mixture-of-Experts

- [x] [Mixtral of Experts](https://arxiv.org/abs/2401.04088)
- [ ] [MegaBlocks: Efficient Sparse Training with Mixture-of-Experts](https://arxiv.org/abs/2211.15841)
- [ ] [GShard](https://arxiv.org/abs/2006.16668)
- [ ] [Switch Transformers](https://arxiv.org/abs/2101.03961)
- [ ] [Mixtures of Experts Unlock Parameter Scaling for Deep RL](https://huggingface.co/papers/2402.08609)
- [ ] [From Sparse to Soft Mixtures of Experts](https://arxiv.org/abs/2308.00951v1)
- [ ] [Pushing Mixture of Experts to the Limit: Extremely Parameter Efficient MoE for Instruction Tuning](https://arxiv.org/abs/2309.05444)
- [ ] [SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention](https://huggingface.co/papers/2312.07987)
- [ ] [MoE-Infinity: : Activation-Aware Expert Offloading for Efficient MoE Serving](https://arxiv.org/abs/2401.14361)
- [ ] [MoE-Mamba: Efficient Selective State Space Models with Mixture of Experts](https://huggingface.co/papers/2401.04081)
- [ ] [Interpretable Mixture of Experts](https://arxiv.org/abs/2206.02107)
- [ ] [A Review of Sparse Expert Models in Deep Learning](https://tenstorrent.com/research/a-review-of-sparse-expert-models-in-deep-learning/)


## LLM Inference

- [ ] [Efficient Memory Management for Large Language Model Serving with PagedAttention](https://arxiv.org/abs/2309.06180)
- [ ] [Accelerating LLM Inference with Staged Speculative Decoding](https://arxiv.org/abs/2308.04623)
- [ ] [Prompt Lookup Decoding - GitHub](https://github.com/apoorvumang/prompt-lookup-decoding)
- [ ] [Break the Sequential Dependency of LLM Inference Using Lookahead Decoding](https://lmsys.org/blog/2023-11-21-lookahead-decoding/)
- [ ] [Fast and Expressive LLM Inference with RadixAttention and SGLang](https://lmsys.org/blog/2024-01-17-sglang/)
- [ ] [Efficient LLM inference solution on Intel GPU](https://arxiv.org/abs/2401.05391)
- [ ] [MatFormer: Nested Transformer for Elastic Inference](https://arxiv.org/abs/2310.07707)
- [ ] [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://huggingface.co/papers/2401.15077)
- [ ] [Specialized Language Models with Cheap Inference from Limited Domain Data](https://huggingface.co/papers/2402.01093)
- [ ] [Accelerating Self-Attentions for LLM Serving with FlashInfer](https://flashinfer.ai/2024/02/02/introduce-flashinfer.html)
- [ ] [Beyond Chinchilla-Optimal: Accounting for Inference in Language Model Scaling Laws](https://arxiv.org/abs/2401.00448)
- [ ] [Tandem Transformers for Inference Efficient LLMs](https://arxiv.org/abs/2402.08644)


## Models

- [ ] [Scalable Pre-training of Large Autoregressive Image Models](https://arxiv.org/abs/2401.08541)
- [ ] [DeepMind - GraphCast](https://github.com/google-deepmind/graphcast)
- [ ] [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- [ ] [Generative Modeling by Estimating Gradients of the Data Distribution](https://yang-song.net/blog/2021/score/)
- [ ] [Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)


## ML Systems

- [ ] [Efficiently Scaling Transformer Inference](https://arxiv.org/abs/2211.05102)
- [ ] [Inside the Matrix: Visualizing Matrix Multiplication, Attention and Beyond](https://pytorch.org/blog/inside-the-matrix/)
- [ ] [Accelerating Generative AI with PyTorch II: GPT, Fast](https://pytorch.org/blog/accelerating-generative-ai-2/)
- [ ] [LLM Training and Inference with Intel Gaudi 2 AI Accelerators](https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators)
- [ ] [How Hugging Face improved Text Generation performance with XLA](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html)
- [ ] [Gemini: Mapping and Architecture Co-exploration for Large-scale DNN Chiplet Accelerators](https://arxiv.org/abs/2312.16436)
- [ ] [The Case for Co-Designing Model Architectures with Hardware](https://arxiv.org/abs/2401.14489)


## Hardware Accelerators

- [ ] [In-Datacenter Performance Analysis of a Tensor Processing Unit](https://arxiv.org/ftp/arxiv/papers/1704/1704.04760.pdf)
- [ ] [A Domain-specific Supercomputer for Training Deep Neural Networks](https://dl.acm.org/doi/pdf/10.1145/3360307)
- [ ] [Ten Lessons From Three Generations Shaped Googleâ€™s TPUv4i](https://gwern.net/doc/ai/scaling/hardware/2021-jouppi.pdf)
- [ ] [Accelerated Computing with a Reconfigurable Dataflow Architecture - SambaNova](https://sambanova.ai/wp-content/uploads/2021/04/SambaNova_Accelerated-Computing-with-a-Reconfigurable-Dataflow-Architecture_Whitepaper_English.pdf)
- [ ] [Think Fast: A Tensor Streaming Processor (TSP) for Accelerating Deep Learning Workloads](http://pkamath.com/publications/papers/tsp-isca20.pdf)
- [ ] [Dissecting the Graphcore IPU Architecture via Microbenchmarking](https://www.graphcore.ai/hubfs/assets/pdf/Citadel%20Securities%20Technical%20Report%20-%20Dissecting%20the%20Graphcore%20IPU%20Architecture%20via%20Microbenchmarking%20Dec%202019.pdf)


## Training Techniques

- [ ] [Small-scale proxies for large-scale Transformer training instabilities](https://arxiv.org/abs/2309.14322)
- [ ] [Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer](https://arxiv.org/abs/2203.03466)
- [ ] [Cramming: Training a Language Model on a Single GPU in One Day](https://arxiv.org/abs/2212.14034)
- [ ] [How to Train Data-Efficient LLMs](https://arxiv.org/abs/2402.09668)
- [ ] [Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning](https://arxiv.org/abs/2205.05638)
- [ ] [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [ ] [Tutorial #17: Transformers III Training](https://www.borealisai.com/research-blogs/tutorial-17-transformers-iii-training/)


## Retrieval Augmented Generation

- [ ] [Building a Retrieval Augmented Generation Chatbot with LangChain and Panel](https://sophiamyang.medium.com/building-a-retrieval-augmented-generation-chatbot-d567a24fcd14)
- [ ] [Panel AI Chatbot Tips: Memory and Downloadable Conversations](https://blog.holoviz.org/posts/ai_chatbot_tips_memory_download/)
- [ ] [The Novice's LLM Training Guide](https://rentry.org/llm-training)
- [ ] [Retrieval Agumented Generation - Prompt Engineering Guide](https://www.promptingguide.ai/techniques/rag)
- [ ] [Intro to Information Retrieval Book](https://nlp.stanford.edu/IR-book/information-retrieval-book.html)
- [ ] [REPLUG: Retrieval-Augmented Black-Box Language Models](https://arxiv.org/abs/2301.12652)
- [ ] [Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In](https://arxiv.org/abs/2305.17331)
- [ ] [Retrieve Anything To Augment Large Language Models](https://arxiv.org/abs/2310.07554)
- [ ] [RAG-Maestro](https://github.com/AymenKallala/RAG_Maestro)
- [ ] [RAG vs. Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture](https://huggingface.co/papers/2401.08406)
- [ ] [LlamaIndex: Enhancing Retrieval Performance with Alpha Tuning in Hybrid Search in RAG](https://blog.llamaindex.ai/llamaindex-enhancing-retrieval-performance-with-alpha-tuning-in-hybrid-search-in-rag-135d0c9b8a00)
- [ ] [UltraTextbooks](https://huggingface.co/datasets/Locutusque/UltraTextbooks)
- [ ] [Retrieval Augmented Generation (RAG) for LLMs - Prompt Engineering Guide](https://www.promptingguide.ai/research/rag)
- [ ] [How we got fine-tuning Mistral-7B to not suck: Helix Project Report, Feb 2024](https://helixml.substack.com/p/how-we-got-fine-tuning-mistral-7b)


## Compression

- [ ] [HEAT: Hardware-Efficient Automatic Tensor Decomposition for Transformer Compression](https://arxiv.org/abs/2211.16749)
- [ ] [A White Paper on Neural Network Quantization](https://arxiv.org/abs/2106.08295)
- [ ] [SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models](https://arxiv.org/abs/2211.10438)
- [ ] [Accelerating Triton Dequantization Kernels for GPTQ](https://pytorch.org/blog/accelerating-triton/)


## Compilers

- [ ] [A Catalogue of Optimizing Transformations](https://www.clear.rice.edu/comp512/Lectures/Papers/1971-allen-catalog.pdf)
- [ ] [21 compilers and 3 orders of magnitude in 60 minutes](http://venge.net/graydon/talks/CompilerTalk-2019.pdf)
- [ ] [Graphene: An IR for Optimized Tensor Computations on GPUs](https://dl.acm.org/doi/abs/10.1145/3582016.3582018)


## Repos

- [ ] [Kaggle Contest: Google - Fast or Slow? Predict AI Model Runtime](https://www.kaggle.com/competitions/predict-ai-model-runtime)
- [ ] [GitHub: apple/ml-vision-transformers-ane](https://github.com/apple/ml-vision-transformers-ane)
- [ ] [Open Collaborative Research: High Order Moment Models - David Picard](https://github.com/davidpicard/HoMM)


## Miscellaneous

- [ ] [How to Optimize a CUDA Matmul Kernel for cuBLAS-like Performance: a Worklog](https://siboehm.com/articles/22/CUDA-MMM)
- [ ] [The Efficiency Spectrum of Large Language Models: An Algorithmic Survey](https://arxiv.org/abs/2312.00678)
- [ ] [Combining Axes Preconditioners through Kronecker Approximation for Deep Learning](https://openreview.net/forum?id=8j9hz8DVi8)
- [ ] [Automatic differentiation in PyTorch](https://openreview.net/forum?id=BJJsrmfCZ)
- [ ] [Simple and Controllable Music Generation](https://arxiv.org/abs/2306.05284)
- [ ] [Accelerating Reduction and Scan Using Tensor Core Units](https://arxiv.org/abs/1811.09736)
- [ ] [How to Escape Saddle Points Efficiently](https://arxiv.org/abs/1703.00887)
- [ ] [How to Scale Hyperparameters as Batch Size Increases](https://www.cs.princeton.edu/~smalladi/blog/2024/01/22/SDEs-ScalingRules/)
- [ ] [Model Merging Paper Collection - Hugging Face](https://huggingface.co/collections/osanseviero/model-merging-65097893623330a3a51ead66)
- [ ] [Knowledge Fusion of Large Language Models](https://arxiv.org/abs/2401.10491)
- Datalogy
  - [ ] [Introduction blog](https://www.datologyai.com/post/introducing-datologyai-making-models-better-through-better-data-automatically)
  - [ ] [D4: Improving LLM Pretraining via Document De-Duplication and Diversification](https://arxiv.org/abs/2308.12284)
  - [ ] [Beyond neural scaling laws: beating power law scaling via data pruning](https://arxiv.org/abs/2206.14486)
